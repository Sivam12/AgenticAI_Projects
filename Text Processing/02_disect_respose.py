from google.generativeai import GenerativeModel
from utilities import disect_response

# candidates     | List of possible answers generated by Gemini.
# avg_logprobs   | Model's confidence score based on average log probabilities of generated tokens.
# content        | The actual text content generated by the model.
# finish_reason  | Reason why the model stopped generating (e.g., normal stop, length limit).
# safety_ratings | Modelâ€™s internal safety evaluation across categories like hate speech, harassment, etc.
#                  HATE SPEECH, DANGEROUS CONTENT, HARASSMENT, SEXUALLY EXPLICIT.
# model_version  | The specific version of the Gemini model used for the response.
# usage_metadata | Token counts showing how much prompt and response text was processed (important for billing).


def main():
    model = GenerativeModel("gemini-2.0-flash-001")
    response = model.generate_content("Who is the Prime minister of India")

    disect_response(response)

    response = model.generate_content("Who is the Prime minister of India")
    disect_response(response)

if __name__ == "__main__":
    main()

